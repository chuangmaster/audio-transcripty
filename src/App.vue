<template>
  <div class="app-container">
    <header class="app-header">
      <h1>ğŸ™ï¸ æœƒè­°éŒ„éŸ³èˆ‡è½‰æ–‡å­—æœå‹™</h1>
      <p>è¼•é¬†å°‡èªéŸ³è½‰æ›ç‚ºæ–‡å­—ï¼Œæå‡å·¥ä½œæ•ˆç‡</p>
      
      <!-- API ç‹€æ…‹æŒ‡ç¤ºå™¨ -->
      <div class="api-status-banner">
        <span v-if="apiStatus.initialized" class="status-badge success">
          âœ“ API æœå‹™å·²é€£æ¥
        </span>
        <span v-else-if="apiStatus.checking" class="status-badge loading">
          â³ æ­£åœ¨åˆå§‹åŒ– API...
        </span>
        <span v-else class="status-badge error">
          âœ— API é€£æ¥å¤±æ•—: {{ apiStatus.error }}
        </span>
        
        <!-- API è©³ç´°ç‹€æ…‹ -->
        <div class="api-details">
          <span :class="['api-item', { active: apiStatus.webSpeechAvailable }]">
            Web Speech: {{ apiStatus.webSpeechAvailable ? 'âœ“' : 'âœ—' }}
          </span>
          <span :class="['api-item', { active: apiStatus.googleAvailable }]">
            Google: {{ apiStatus.googleAvailable ? 'âœ“' : 'âœ—' }}
          </span>
          <span :class="['api-item', { active: apiStatus.openaiAvailable }]">
            OpenAI: {{ apiStatus.openaiAvailable ? 'âœ“' : 'âœ—' }}
          </span>
        </div>
        
          <!-- ç•¶ä»»ä¸€ API æœªé€£ç·šæ™‚ï¼Œé¡¯ç¤ºè¼¸å…¥æ¡†è®“ä½¿ç”¨è€…æ‰‹å‹•è¼¸å…¥é‡‘é‘° -->
          <div class="api-inputs">
            <div v-if="!apiStatus.googleAvailable" class="api-input">
              <label>Google API Key</label>
              <input type="text" v-model="googleKeyInput" placeholder="è¼¸å…¥ Google API Key" />
              <button class="btn btn-secondary" @click="saveGoogleKey" :disabled="!googleKeyInput || apiStatus.checking">å„²å­˜ä¸¦é‡è©¦</button>
            </div>

            <div v-if="!apiStatus.openaiAvailable" class="api-input">
              <label>OpenAI API Key</label>
              <input type="text" v-model="openaiKeyInput" placeholder="è¼¸å…¥ OpenAI API Key" />
              <button class="btn btn-secondary" @click="saveOpenAIKey" :disabled="!openaiKeyInput || apiStatus.checking">å„²å­˜ä¸¦é‡è©¦</button>
            </div>
          </div>
      </div>

      <!-- å¼•æ“é¸æ“‡å™¨ -->
      <div class="engine-selector">
        <label>é¸æ“‡è½‰æ›å¼•æ“ï¼š</label>
        <div class="engine-options">
          <button 
            v-for="engine in availableEngines" 
            :key="engine.type"
            @click="selectedEngine = engine.type"
            :class="['engine-btn', { active: selectedEngine === engine.type }]"
            :disabled="!engine.available"
            :title="engine.available ? '' : engine.unavailableReason"
          >
            <span class="engine-name">{{ engine.name }}</span>
            <span class="engine-desc">{{ engine.description }}</span>
            <span v-if="engine.free" class="engine-free">ğŸ†“ å…è²»</span>
          </button>
        </div>
      </div>

      <!-- æ‘˜è¦æ¨¡å‹é¸æ“‡ -->
      <div class="summary-model-selector">
        <label>é¸æ“‡æ‘˜è¦æ¨¡å‹ï¼š</label>
        <div class="model-options">
          <button 
            v-for="m in summaryModelsOptions" 
            :key="m.value"
            @click="summaryModel = m.value"
            :class="['model-btn', { active: summaryModel === m.value }]"
            :title="m.value"
          >
            <span class="model-name">{{ m.label }}</span>
          </button>
        </div>
      </div>
    </header>

    <main class="main-content">
      <!-- éŒ„éŸ³æ§åˆ¶å€ -->
      <div class="recording-section">
        <div class="recording-controls">
          <button 
            v-if="!isRecording" 
            @click="startRecording" 
            class="btn btn-primary"
            :disabled="isProcessing"
          >
            <span class="icon">â—</span> é–‹å§‹éŒ„éŸ³
          </button>
          <button 
            v-else 
            @click="stopRecording" 
            class="btn btn-danger"
          >
            <span class="icon">â– </span> åœæ­¢éŒ„éŸ³
          </button>
          
          <div v-if="isRecording" class="recording-indicator">
            <span class="pulse"></span>
            <span>éŒ„éŸ³ä¸­... {{ recordingTime }}</span>
          </div>
        </div>

        <!-- æª”æ¡ˆä¸Šå‚³å€ -->
        <div class="upload-section">
          <div class="upload-label">æˆ–ä¸Šå‚³éŸ³è¨Šæª”æ¡ˆï¼š</div>
          <label class="upload-btn">
            <span class="icon">ğŸ“</span> é¸æ“‡æª”æ¡ˆ
            <input 
              type="file" 
              @change="handleFileUpload" 
              accept="audio/*"
              :disabled="isProcessing || isRecording"
              style="display: none"
            />
          </label>
          <span v-if="selectedFile" class="file-info">
            ğŸ“„ {{ selectedFile.name }} ({{ formatFileSize(selectedFile.size) }})
          </span>
        </div>

        <!-- èªè¨€é¸æ“‡ -->
        <div class="language-selector">
          <label for="language">é¸æ“‡èªè¨€ï¼š</label>
          <select id="language" v-model="selectedLanguage" :disabled="isRecording">
            <option value="zh-TW">ç¹é«”ä¸­æ–‡</option>
            <option value="zh-CN">ç°¡é«”ä¸­æ–‡</option>
            <option value="en-US">English (US)</option>
            <option value="ja-JP">æ—¥æœ¬èª</option>
            <option value="ko-KR">í•œêµ­ì–´</option>
          </select>
        </div>
      </div>

      <!-- è™•ç†ä¸­æç¤º -->
      <div v-if="isProcessing" class="processing">
        <div class="spinner"></div>
        <p>{{ processingStatus || 'æ­£åœ¨è™•ç†èªéŸ³è½‰æ–‡å­—...' }}</p>
        <div class="progress-bar">
          <div class="progress-fill" :style="{ width: processingProgress + '%' }"></div>
        </div>
        <span class="progress-text">{{ processingProgress }}%</span>
      </div>

      <!-- éŒ„éŸ³åˆ—è¡¨ -->
      <div v-if="recordings.length > 0" class="recordings-list">
        <h2>éŒ„éŸ³è¨˜éŒ„</h2>
          <div v-for="(recording, index) in recordings" :key="index" class="recording-item">
          <div class="recording-header">
            <h3>éŒ„éŸ³ {{ recordings.length - index }}</h3>
            <span class="recording-date">{{ recording.date }}</span>
            <span v-if="recording.engine" class="engine-tag">{{ getEngineLabel(recording.engine) }}</span>
          </div>          <!-- éŸ³è¨Šæ’­æ”¾å™¨ -->
          <div v-if="recording.audioUrl" class="audio-player">
            <audio :src="recording.audioUrl" controls></audio>
          </div>
          <div v-else class="audio-player no-audio">
            <p>{{ (recording.engine === 'webSpeech' || recording.engine === 'openai') ? '(ç„¡æ³•å–å¾—éŸ³è¨Š)' : '(éŸ³è¨Šæœªä¿å­˜)' }}</p>
          </div>

          <!-- è½‰æ›å¾Œçš„æ–‡å­— -->
          <div v-if="recording.transcript" class="transcript">
            <h4>è½‰æ›æ–‡å­—ï¼š</h4>
            <div class="transcript-content">{{ recording.transcript }}</div>
            
            <!-- æ‘˜è¦ -->
            <div v-if="recording.summary" class="summary">
              <h4>æ‘˜è¦ï¼š</h4>
              <p>{{ recording.summary }}</p>
            </div>
          </div>

          <!-- æ“ä½œæŒ‰éˆ• -->
          <div class="recording-actions">
            <button @click="downloadTranscript(recording)" class="btn btn-secondary" :disabled="!recording.transcript">
              <span class="icon">â¬‡</span> ä¸‹è¼‰æ–‡å­—
            </button>
            <button @click="generateSummary(recording)" class="btn btn-secondary" :disabled="!recording.transcript || recording.summary">
              <span class="icon">ğŸ“</span> ç”Ÿæˆæ‘˜è¦
            </button>
            <button @click="deleteRecording(index)" class="btn btn-danger-outline">
              <span class="icon">ğŸ—‘ï¸</span> åˆªé™¤
            </button>
          </div>
        </div>
      </div>

      <!-- ç©ºç‹€æ…‹ -->
      <div v-else class="empty-state">
        <p>å°šç„¡éŒ„éŸ³è¨˜éŒ„ï¼Œé»æ“Šã€Œé–‹å§‹éŒ„éŸ³ã€é–‹å§‹ä½¿ç”¨</p>
      </div>
    </main>

    <!-- éŒ¯èª¤è¨Šæ¯ -->
    <div v-if="errorMessage" class="error-toast" @click="errorMessage = ''">
      {{ errorMessage }}
    </div>
  </div>
</template>

<script setup>
import { ref, onUnmounted, reactive, computed } from 'vue'
import SpeechToTextService from './services/speechToText.js'
import { WebSpeechApiService } from './services/webSpeechApi.js'
import OpenAIWhisperService from './services/openaiWhisper.js'
import SummarizeService from './services/summarize.js'
import GoogleSummarizeService from './services/googleSummarize.js'
import { speechToTextConfig } from './config/speechToTextConfig.js'

// API æœå‹™åˆå§‹åŒ–
let speechToTextService = null
let webSpeechService = null
let openaiWhisperService = null
let summarizeService = null
let googleSummarizeService = null
const apiStatus = reactive({
  initialized: false,
  error: null,
  checking: false,
  openaiAvailable: false,
  googleAvailable: false,
  webSpeechAvailable: false,
})

// åˆå§‹åŒ– API æœå‹™
const initializeSpeechService = async () => {
  try {
    apiStatus.checking = true
    
    // åˆå§‹åŒ– Web Speech API
    webSpeechService = new WebSpeechApiService()
    apiStatus.webSpeechAvailable = webSpeechService.isSupported()
    console.log('âœ“ Web Speech API å·²åˆå§‹åŒ–', apiStatus.webSpeechAvailable ? '(æ”¯æ´)' : '(ä¸æ”¯æ´)')
    
    // åˆå§‹åŒ– Google Speech-to-Text API
    const googleKey = speechToTextConfig.googleApiKey
    if (!googleKey || googleKey === 'YOUR_API_KEY_HERE') {
      console.warn('âš ï¸ Google Speech-to-Text API å¯†é‘°æœªè¨­å®š')
      apiStatus.googleAvailable = false
    } else {
      try {
        speechToTextService = new SpeechToTextService(googleKey)
        const isValid = await speechToTextService.validateApiKey()
        apiStatus.googleAvailable = isValid
        if (isValid) {
          console.log('âœ“ Google Speech-to-Text API å·²é©—è­‰')
        } else {
          console.warn('âš ï¸ Google API å¯†é‘°ç„¡æ•ˆ')
          speechToTextService = null
        }
      } catch (error) {
        console.error('âœ— Google API åˆå§‹åŒ–å¤±æ•—:', error.message)
        speechToTextService = null
        apiStatus.googleAvailable = false
      }
      // ç„¡è«– Speech-to-Text æ˜¯å¦èƒ½åˆå§‹åŒ–ï¼Œåªè¦æœ‰ Google Key å°±å¯ä»¥å»ºç«‹ GoogleSummarizeService
      try {
        googleSummarizeService = new GoogleSummarizeService(googleKey)
        console.log('GoogleSummarizeService å·²å»ºç«‹')
      } catch (e) {
        console.warn('ç„¡æ³•å»ºç«‹ GoogleSummarizeService:', e.message)
        googleSummarizeService = null
      }
    }

    // åˆå§‹åŒ– OpenAI Whisper API
    const openaiKey = speechToTextConfig.openaiApiKey
    console.log('=== OpenAI API åˆå§‹åŒ– ===')
    console.log('OpenAI Key ç‹€æ…‹:', openaiKey ? 'å·²è¨­å®š (é•·åº¦: ' + openaiKey.length + ')' : 'æœªè¨­å®š')
    console.log('Key æœ‰æ•ˆæ€§æª¢æŸ¥:', openaiKey && openaiKey !== 'YOUR_API_KEY_HERE' ? 'é€šé' : 'å¤±æ•—')
    
    if (!openaiKey || openaiKey === 'YOUR_API_KEY_HERE') {
      console.warn('âš ï¸ OpenAI API å¯†é‘°æœªè¨­å®š - å°‡ç¦ç”¨ OpenAI å¼•æ“')
      apiStatus.openaiAvailable = false
    } else {
      try {
        console.log('ğŸ“¡ æ­£åœ¨é©—è­‰ OpenAI API å¯†é‘°...')
        openaiWhisperService = new OpenAIWhisperService(openaiKey)
        const isValid = await openaiWhisperService.validateApiKey()
        apiStatus.openaiAvailable = isValid
        console.log('API é©—è­‰çµæœ:', isValid ? 'âœ“ æˆåŠŸ' : 'âœ— å¤±æ•—')
        
        if (isValid) {
          console.log('âœ“ OpenAI Whisper API å·²é©—è­‰ä¸¦åˆå§‹åŒ–æˆåŠŸ')
        } else {
          console.warn('âš ï¸ OpenAI API å¯†é‘°ç„¡æ•ˆæˆ–å·²éæœŸ')
          openaiWhisperService = null
        }
      } catch (error) {
        console.error('âœ— OpenAI API åˆå§‹åŒ–å¤±æ•—:', error.message)
        console.error('è©³ç´°éŒ¯èª¤:', error)
        openaiWhisperService = null
        apiStatus.openaiAvailable = false
      }
      // ç„¡è«– Whisper æ˜¯å¦èƒ½åˆå§‹åŒ–ï¼Œåªè¦æœ‰ OpenAI Key å°±å¯ä»¥å»ºç«‹ SummarizeService
      try {
        summarizeService = new SummarizeService(openaiKey)
        console.log('SummarizeService å·²å»ºç«‹')
      } catch (e) {
        console.warn('ç„¡æ³•å»ºç«‹ SummarizeService:', e.message)
        summarizeService = null
      }
    }
    console.log('=== OpenAI åˆå§‹åŒ–å®Œæˆ ===')
    console.log('OpenAI æœå‹™ç‹€æ…‹:', openaiWhisperService ? 'âœ“ å·²åˆå§‹åŒ–' : 'âœ— æœªåˆå§‹åŒ–')

    // è¨­å®šåˆå§‹åŒ–ç‹€æ…‹ (åªè¦æœ‰ä»»ä½• API å¯ç”¨å°±ç®—åˆå§‹åŒ–æˆåŠŸ)
    const hasApiAvailable = apiStatus.googleAvailable || apiStatus.openaiAvailable || apiStatus.webSpeechAvailable
    apiStatus.initialized = hasApiAvailable
    apiStatus.error = hasApiAvailable ? null : 'ç„¡å¯ç”¨çš„ API æœå‹™'
    
    console.log('API åˆå§‹åŒ–å®Œæˆ:', {
      webSpeech: apiStatus.webSpeechAvailable,
      google: apiStatus.googleAvailable,
      openai: apiStatus.openaiAvailable,
      initialized: apiStatus.initialized
    })
  } catch (error) {
    console.error('âœ— åˆå§‹åŒ–å¤±æ•—:', error)
    apiStatus.error = error.message || 'ç„¡æ³•åˆå§‹åŒ– API æœå‹™'
    apiStatus.initialized = false
  } finally {
    apiStatus.checking = false
  }
}

// åœ¨å…ƒä»¶æ›è¼‰æ™‚åˆå§‹åŒ–
if (import.meta.hot) {
  import.meta.hot.accept()
}
initializeSpeechService()

// ç‹€æ…‹ç®¡ç†
const isRecording = ref(false)
const isProcessing = ref(false)
const recordings = ref([])
const selectedLanguage = ref('zh-TW')
const selectedEngine = ref('webSpeech') // é è¨­ä½¿ç”¨ Web Speech API
// æ‘˜è¦æ¨¡å‹è¨­å®š
const summaryModel = ref('gpt-3.5-turbo')
const summaryModelsOptions = ref([
  { value: 'gpt-4o-mini', label: 'OpenAI: gpt-4o-mini (å¿«é€Ÿ)' },
  { value: 'gpt-4o', label: 'OpenAI: gpt-4o (é«˜å“è³ª)' },
  { value: 'gpt-3.5-turbo', label: 'OpenAI: gpt-3.5-turbo (æˆæœ¬è¼ƒä½)' },
  { value: 'gemini-2.5-flash-lite', label: 'Google: Gemini 2.5 Flash Lite (æˆæœ¬ä½ã€å¿«é€Ÿ)' },
  { value: 'gemini-2.5-flash', label: 'Google: Gemini 2.5 Flash (é«˜å“è³ª)' }
])
const recordingTime = ref('00:00')
const errorMessage = ref('')
const processingStatus = ref('')
const processingProgress = ref(0)
const selectedFile = ref(null) // ä¸Šå‚³çš„æª”æ¡ˆ
// API key inputs (allow user to enter keys at runtime)
const googleKeyInput = ref(speechToTextConfig.googleApiKey && speechToTextConfig.googleApiKey !== 'YOUR_API_KEY_HERE' ? speechToTextConfig.googleApiKey : '')
const openaiKeyInput = ref(speechToTextConfig.openaiApiKey && speechToTextConfig.openaiApiKey !== 'YOUR_API_KEY_HERE' ? speechToTextConfig.openaiApiKey : '')

const saveGoogleKey = async () => {
  if (!googleKeyInput.value) return
  // æ›´æ–°é…ç½®ä¸¦é‡æ–°åˆå§‹åŒ–
  speechToTextConfig.googleApiKey = googleKeyInput.value
  apiStatus.checking = true
  apiStatus.error = null
  await initializeSpeechService()
}

const saveOpenAIKey = async () => {
  if (!openaiKeyInput.value) return
  speechToTextConfig.openaiApiKey = openaiKeyInput.value
  apiStatus.checking = true
  apiStatus.error = null
  await initializeSpeechService()
}

// è¨ˆç®—å¯ç”¨å¼•æ“
const availableEngines = computed(() => {
  const engines = []
  
  // Web Speech API ç¸½æ˜¯å¯ç”¨
  engines.push({
    type: 'webSpeech',
    name: 'âš¡ å¿«é€Ÿè½‰æ›',
    description: 'Web Speech API - å…è²»ã€ç„¡é™é•·åº¦',
    available: apiStatus.webSpeechAvailable,
    free: true,
    unavailableReason: apiStatus.webSpeechAvailable ? 'å¯ç”¨' : 'æ­¤ç€è¦½å™¨ä¸æ”¯æ´ Web Speech API'
  })
  
  // Google Speech-to-Text API
  engines.push({
    type: 'google',
    name: 'ğŸ¯ ç²¾æº–è½‰æ›',
    description: 'Google Speech-to-Text - é«˜ç²¾åº¦',
    available: apiStatus.googleAvailable,
    free: false,
    unavailableReason: apiStatus.googleAvailable ? 'å¯ç”¨' : 'Google API å¯†é‘°æœªè¨­å®šæˆ–ç„¡æ•ˆ'
  })

  // OpenAI Whisper API
  engines.push({
    type: 'openai',
    name: 'ğŸš€ è¶…å¼·è½‰æ›',
    description: 'OpenAI Whisper - ç„¡é™é•·åº¦ã€è‡ªå‹•åˆ†å‰²',
    available: apiStatus.openaiAvailable,
    free: false,
    unavailableReason: apiStatus.openaiAvailable ? 'å¯ç”¨' : 'OpenAI API å¯†é‘°æœªè¨­å®šæˆ–ç„¡æ•ˆ'
  })
  
  return engines
})

// éŒ„éŸ³ç›¸é—œè®Šæ•¸
let mediaRecorder = null
let audioChunks = []
let recordingTimer = null
let recordingStartTime = 0

// é–‹å§‹éŒ„éŸ³
const startRecording = async () => {
  try {
    if (selectedEngine.value === 'webSpeech') {
      // ä½¿ç”¨ Web Speech API
      startWebSpeechRecording()
    } else if (selectedEngine.value === 'google') {
      // ä½¿ç”¨ Google çš„å‚³çµ±éŒ„éŸ³æ–¹å¼
      startGoogleRecording()
    } else if (selectedEngine.value === 'openai') {
      // ä½¿ç”¨ OpenAI Whisper éŒ„éŸ³ï¼ˆå…ˆéŒ„è£½æœ¬åœ°éŸ³è¨Šï¼Œå†ä¸Šå‚³è‡³ Whisperï¼‰
      startOpenAIRecording()
    }
  } catch (error) {
    console.error('éŒ„éŸ³å¤±æ•—:', error)
    errorMessage.value = 'ç„¡æ³•å­˜å–éº¥å…‹é¢¨ï¼Œè«‹æª¢æŸ¥ç€è¦½å™¨æ¬Šé™è¨­å®š'
  }
}

// Web Speech API éŒ„éŸ³
const startWebSpeechRecording = () => {
  isRecording.value = true
  recordingStartTime = Date.now()
  
  // å…ˆè¨­å®šç‹€æ…‹
  isProcessing.value = true
  processingStatus.value = 'æ­£åœ¨ç›£è½...'
  processingProgress.value = 20

  try {
    webSpeechService.start(
      selectedLanguage.value,
      // å¯¦æ™‚æ›´æ–°å›èª¿
      (update) => {
        processingStatus.value = `æ­£åœ¨è­˜åˆ¥... ä¿¡å¿ƒåº¦: ${(update.confidence * 100).toFixed(0)}%`
        processingProgress.value = 30 + (update.confidence * 50)
      },
      // å®Œæˆå›èª¿
      (result) => {
        isRecording.value = false
        if (recordingTimer) {
          clearInterval(recordingTimer)
          recordingTimer = null
        }
        recordingTime.value = '00:00'
        
        // ä¿å­˜çµæœ
        if (result.transcript && result.transcript.trim()) {
          const recording = {
            transcript: result.transcript,
            audioBlob: null, // Web Speech API ç„¡æ³•å–å¾—éŸ³è¨Š
            audioUrl: null,
            summary: null,
            date: new Date().toLocaleString('zh-TW'),
            language: selectedLanguage.value,
            engine: 'webSpeech'
          }
          
          recordings.value.unshift(recording)
          errorMessage.value = '' // æ¸…é™¤éŒ¯èª¤
        } else {
          errorMessage.value = 'Web Speech API: æœªè­˜åˆ¥åˆ°ä»»ä½•æ–‡å­—'
        }
        
        isProcessing.value = false
        processingStatus.value = ''
        processingProgress.value = 0
      },
      // éŒ¯èª¤å›èª¿
      (error) => {
        isRecording.value = false
        if (recordingTimer) {
          clearInterval(recordingTimer)
          recordingTimer = null
        }
        recordingTime.value = '00:00'
        
        console.error('Web Speech API éŒ¯èª¤:', error)
        errorMessage.value = `Web Speech éŒ¯èª¤: ${error.message}`
        
        isProcessing.value = false
        processingStatus.value = ''
        processingProgress.value = 0
      }
    )
  } catch (error) {
    console.error('å•Ÿå‹• Web Speech API å¤±æ•—:', error)
    errorMessage.value = `å•Ÿå‹•éŒ¯èª¤: ${error.message}`
    isRecording.value = false
    isProcessing.value = false
    if (recordingTimer) {
      clearInterval(recordingTimer)
      recordingTimer = null
    }
  }

  // é–‹å§‹è¨ˆæ™‚
  recordingTimer = setInterval(() => {
    const elapsed = Math.floor((Date.now() - recordingStartTime) / 1000)
    const minutes = Math.floor(elapsed / 60).toString().padStart(2, '0')
    const seconds = (elapsed % 60).toString().padStart(2, '0')
    recordingTime.value = `${minutes}:${seconds}`
  }, 1000)
}

// Google Speech-to-Text éŒ„éŸ³
const startGoogleRecording = async () => {
  const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
  
  mediaRecorder = new MediaRecorder(stream)
  audioChunks = []
  
  mediaRecorder.ondataavailable = (event) => {
    if (event.data.size > 0) {
      audioChunks.push(event.data)
    }
  }
  
  mediaRecorder.onstop = async () => {
    const audioBlob = new Blob(audioChunks, { type: 'audio/webm' })
    const audioUrl = URL.createObjectURL(audioBlob)
    
    // åœæ­¢æ‰€æœ‰éŸ³è»Œ
    stream.getTracks().forEach(track => track.stop())
    
    // è™•ç†èªéŸ³è½‰æ–‡å­—
    await processAudioToText(audioBlob, audioUrl)
  }
  
  mediaRecorder.start()
  isRecording.value = true
  recordingStartTime = Date.now()
  
  // é–‹å§‹è¨ˆæ™‚
  recordingTimer = setInterval(() => {
    const elapsed = Math.floor((Date.now() - recordingStartTime) / 1000)
    const minutes = Math.floor(elapsed / 60).toString().padStart(2, '0')
    const seconds = (elapsed % 60).toString().padStart(2, '0')
    recordingTime.value = `${minutes}:${seconds}`
  }, 1000)
}

// OpenAI éŒ„éŸ³ï¼šèˆ‡ Google é¡ä¼¼ï¼ŒéŒ„è£½éŸ³è¨Šç„¶å¾Œä¸Šå‚³åˆ° OpenAI Whisper
const startOpenAIRecording = async () => {
  if (!openaiWhisperService) {
    errorMessage.value = 'OpenAI Whisper æœå‹™æœªåˆå§‹åŒ–æˆ–é‡‘é‘°ç„¡æ•ˆ'
    return
  }

  const stream = await navigator.mediaDevices.getUserMedia({ audio: true })

  mediaRecorder = new MediaRecorder(stream)
  audioChunks = []

  mediaRecorder.ondataavailable = (event) => {
    if (event.data.size > 0) {
      audioChunks.push(event.data)
    }
  }

  mediaRecorder.onstop = async () => {
    const audioBlob = new Blob(audioChunks, { type: 'audio/webm' })
    const audioUrl = URL.createObjectURL(audioBlob)

    // åœæ­¢æ‰€æœ‰éŸ³è»Œ
    stream.getTracks().forEach(track => track.stop())

    // ä½¿ç”¨ OpenAI Whisper è½‰éŒ„
    await processAudioToTextOpenAI(audioBlob, audioUrl)
  }

  mediaRecorder.start()
  isRecording.value = true
  recordingStartTime = Date.now()

  // é–‹å§‹è¨ˆæ™‚
  recordingTimer = setInterval(() => {
    const elapsed = Math.floor((Date.now() - recordingStartTime) / 1000)
    const minutes = Math.floor(elapsed / 60).toString().padStart(2, '0')
    const seconds = (elapsed % 60).toString().padStart(2, '0')
    recordingTime.value = `${minutes}:${seconds}`
  }, 1000)
}

// åœæ­¢éŒ„éŸ³
const stopRecording = () => {
  if (selectedEngine.value === 'webSpeech') {
    webSpeechService.stop()
    isRecording.value = false
  } else if (selectedEngine.value === 'google') {
    if (mediaRecorder && mediaRecorder.state !== 'inactive') {
      mediaRecorder.stop()
      isRecording.value = false
    }
  } else if (selectedEngine.value === 'openai') {
    if (mediaRecorder && mediaRecorder.state !== 'inactive') {
      mediaRecorder.stop()
      isRecording.value = false
    }
  }
  
  if (recordingTimer) {
    clearInterval(recordingTimer)
    recordingTimer = null
  }
  recordingTime.value = '00:00'
}

// ä½¿ç”¨ OpenAI Whisper è½‰éŒ„éŒ„è£½å¥½çš„éŸ³è¨Š
const processAudioToTextOpenAI = async (audioBlob, audioUrl) => {
  if (!apiStatus.initialized || !openaiWhisperService) {
    errorMessage.value = 'OpenAI Whisper æœå‹™æœªåˆå§‹åŒ–ï¼Œè«‹æª¢æŸ¥é…ç½®'
    const recording = {
      audioUrl,
      audioBlob,
      transcript: '[ç­‰å¾… OpenAI åˆå§‹åŒ–]',
      summary: null,
      date: new Date().toLocaleString('zh-TW'),
      language: selectedLanguage.value,
      engine: 'openai'
    }
    recordings.value.unshift(recording)
    return
  }

  isProcessing.value = true
  processingStatus.value = 'æº–å‚™éŸ³è¨Šæª”æ¡ˆ...'
  processingProgress.value = 0

  try {
    const languageCode = selectedLanguage.value.split('-')[0]
    const transcript = await openaiWhisperService.transcribeAudio(
      audioBlob,
      languageCode,
      (progress) => {
        processingStatus.value = progress.status
        processingProgress.value = progress.progress
      }
    )

    const recording = {
      audioUrl,
      audioBlob,
      transcript,
      summary: null,
      date: new Date().toLocaleString('zh-TW'),
      language: selectedLanguage.value,
      engine: 'openai'
    }

    recordings.value.unshift(recording)
    errorMessage.value = ''
  } catch (error) {
    console.error('OpenAI è½‰æ›å¤±æ•—:', error)
    errorMessage.value = `èªéŸ³è½‰æ–‡å­—å¤±æ•—: ${error.message}`

    const recording = {
      audioUrl,
      audioBlob,
      transcript: `[è½‰æ›å¤±æ•—] ${error.message}`,
      summary: null,
      date: new Date().toLocaleString('zh-TW'),
      language: selectedLanguage.value,
      engine: 'openai'
    }
    recordings.value.unshift(recording)
  } finally {
    isProcessing.value = false
    processingStatus.value = ''
    processingProgress.value = 0
  }
}

// è™•ç†èªéŸ³è½‰æ–‡å­—
const processAudioToText = async (audioBlob, audioUrl) => {
  if (!apiStatus.initialized) {
    errorMessage.value = 'Google API æœå‹™æœªåˆå§‹åŒ–ï¼Œè«‹æª¢æŸ¥é…ç½®'
    // å³ä½¿ API åˆå§‹åŒ–å¤±æ•—ï¼Œä»ä¿å­˜éŒ„éŸ³
    const recording = {
      audioUrl,
      audioBlob,
      transcript: '[ç­‰å¾… API åˆå§‹åŒ–]',
      summary: null,
      date: new Date().toLocaleString('zh-TW'),
      language: selectedLanguage.value,
      engine: 'google'
    }
    recordings.value.unshift(recording)
    return
  }

  isProcessing.value = true
  processingStatus.value = 'æº–å‚™éŸ³è¨Šæª”æ¡ˆ...'
  processingProgress.value = 0

  try {
    // è½‰æ›éŸ³è¨Šæª”æ¡ˆï¼ˆå¦‚éœ€è¦ï¼‰
    const processedAudio = await speechToTextService.convertAudioFormat(audioBlob)

    // ä½¿ç”¨ Google Speech-to-Text API è½‰æ›
    const transcript = await speechToTextService.transcribeAudio(
      processedAudio,
      selectedLanguage.value,
      (progress) => {
        processingStatus.value = progress.status
        processingProgress.value = progress.progress
      }
    )

    const recording = {
      audioUrl,
      audioBlob,
      transcript,
      summary: null,
      date: new Date().toLocaleString('zh-TW'),
      language: selectedLanguage.value,
      engine: 'google'
    }

    recordings.value.unshift(recording)
  } catch (error) {
    console.error('è½‰æ›å¤±æ•—:', error)
    errorMessage.value = `èªéŸ³è½‰æ–‡å­—å¤±æ•—: ${error.message}`

    // å³ä½¿è½‰æ›å¤±æ•—ï¼Œä»ä¿å­˜éŒ„éŸ³
    const recording = {
      audioUrl,
      audioBlob,
      transcript: `[è½‰æ›å¤±æ•—] ${error.message}`,
      summary: null,
      date: new Date().toLocaleString('zh-TW'),
      language: selectedLanguage.value,
      engine: 'google'
    }
    recordings.value.unshift(recording)
  } finally {
    isProcessing.value = false
    processingStatus.value = ''
    processingProgress.value = 0
  }
}

// ä½¿ç”¨ Google Speech-to-Text API è½‰æ›èªéŸ³
const transcribeAudio = async (audioBlob) => {
  if (!speechToTextService) {
    throw new Error('API æœå‹™æœªåˆå§‹åŒ–')
  }
  return await speechToTextService.transcribeAudio(audioBlob, selectedLanguage.value)
}

// ä¸‹è¼‰è½‰æ›æ–‡å­—
const downloadTranscript = (recording) => {
  if (!recording.transcript) {
    errorMessage.value = 'æ²’æœ‰å¯ä¸‹è¼‰çš„æ–‡å­—'
    return
  }
  
  try {
    const engineLabel = getEngineLabel(recording.engine)
    const text = `éŒ„éŸ³æ—¥æœŸ: ${recording.date}
èªè¨€: ${recording.language}
å¼•æ“: ${engineLabel}

è½‰æ›æ–‡å­—:
${recording.transcript}

${recording.summary ? 'æ‘˜è¦:\n' + recording.summary : ''}`
    
    const blob = new Blob([text], { type: 'text/plain;charset=utf-8' })
    const url = URL.createObjectURL(blob)
    const a = document.createElement('a')
    a.href = url
    
    // ç”Ÿæˆå®‰å…¨çš„æª”å (ç§»é™¤æ‰€æœ‰ç‰¹æ®Šå­—ç¬¦)
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-').substring(0, 19)
    a.download = `éŒ„éŸ³æ–‡å­—_${timestamp}.txt`
    
    document.body.appendChild(a)
    a.click()
    document.body.removeChild(a)
    
    // å»¶é²é‡‹æ”¾ URLï¼Œç¢ºä¿ä¸‹è¼‰å®Œæˆ
    setTimeout(() => URL.revokeObjectURL(url), 100)
    
    errorMessage.value = '' // æ¸…é™¤ä»»ä½•éŒ¯èª¤
  } catch (error) {
    console.error('ä¸‹è¼‰å¤±æ•—:', error)
    errorMessage.value = `ä¸‹è¼‰å¤±æ•—: ${error.message}`
  }
}

// ç²å–å¼•æ“æ¨™ç±¤
const getEngineLabel = (engine) => {
  const labels = {
    'webSpeech': 'âš¡ Web Speech',
    'google': 'ğŸ¯ Google',
    'openai': 'ğŸš€ OpenAI'
  }
  return labels[engine] || engine
}

// ç”Ÿæˆæ‘˜è¦ï¼ˆä½¿ç”¨ SummarizeService æˆ– GoogleSummarizeServiceï¼‰
const generateSummary = async (recording) => {
  if (!recording.transcript || recording.summary) return

  // åˆ¤æ–·ä½¿ç”¨çš„ API æœå‹™
  const isGoogleModel = summaryModel.value.startsWith('gemini')
  const service = isGoogleModel ? googleSummarizeService : summarizeService

  if (!service) {
    const apiName = isGoogleModel ? 'Google' : 'OpenAI'
    errorMessage.value = `æ‘˜è¦æœå‹™æœªåˆå§‹åŒ–æˆ– ${apiName} API Key æœªè¨­å®š`
    return
  }

  try {
    // é¡¯ç¤ºæš«æ™‚ç‹€æ…‹
    recording.summary = 'æ­£åœ¨ç”Ÿæˆæ‘˜è¦...'
    const prevProcessing = isProcessing.value
    isProcessing.value = true
    processingStatus.value = `ä½¿ç”¨ ${summaryModel.value} ç”Ÿæˆæ‘˜è¦...`
    processingProgress.value = 10

    // å»ºè­°çš„ optionsï¼Œå¯ä¾éœ€æ±‚èª¿æ•´
    const options = {
      temperature: 0.2,
      max_tokens: 400,
      userPrompt: `è«‹ä»¥ç¹é«”ä¸­æ–‡æ‘˜è¦ä»¥ä¸‹å…§å®¹ï¼Œåˆ—å‡ºé‡é»èˆ‡çµè«–ï¼Œç›¡é‡ä»¥æ¢åˆ—æ–¹å¼å‘ˆç¾ï¼š\n\n${recording.transcript}`
    }

    const summary = await service.summarizeText(recording.transcript, summaryModel.value, options)

    recording.summary = summary || '[AI æœªå›å‚³æ‘˜è¦]'
    processingStatus.value = ''
    processingProgress.value = 0
    isProcessing.value = prevProcessing
  } catch (error) {
    console.error('æ‘˜è¦å¤±æ•—:', error)
    recording.summary = `[æ‘˜è¦å¤±æ•—] ${error.message}`
    errorMessage.value = `æ‘˜è¦å¤±æ•—: ${error.message}`
    isProcessing.value = false
    processingStatus.value = ''
    processingProgress.value = 0
  }
}

// åˆªé™¤éŒ„éŸ³
const deleteRecording = (index) => {
  if (confirm('ç¢ºå®šè¦åˆªé™¤é€™ç­†éŒ„éŸ³å—?')) {
    // é‡‹æ”¾ URL
    if (recordings.value[index].audioUrl) {
      URL.revokeObjectURL(recordings.value[index].audioUrl)
    }
    recordings.value.splice(index, 1)
  }
}

// æ ¼å¼åŒ–æª”æ¡ˆå¤§å°
const formatFileSize = (bytes) => {
  if (bytes === 0) return '0 B'
  const k = 1024
  const sizes = ['B', 'KB', 'MB']
  const i = Math.floor(Math.log(bytes) / Math.log(k))
  return Math.round((bytes / Math.pow(k, i)) * 100) / 100 + ' ' + sizes[i]
}

// è™•ç†æª”æ¡ˆä¸Šå‚³
const handleFileUpload = async (event) => {
  const file = event.target.files?.[0]
  if (!file) return
  
  // é©—è­‰æª”æ¡ˆé¡å‹
  if (!file.type.startsWith('audio/')) {
    errorMessage.value = 'è«‹é¸æ“‡æœ‰æ•ˆçš„éŸ³è¨Šæª”æ¡ˆï¼ˆæ”¯æ´ MP3ã€WAVã€WebMã€OGGã€FLAC ç­‰ï¼‰'
    event.target.value = '' // æ¸…é™¤é¸æ“‡
    return
  }
  
  // é©—è­‰æª”æ¡ˆå¤§å° (é™åˆ¶ 100MB)
  if (file.size > 100 * 1024 * 1024) {
    errorMessage.value = 'æª”æ¡ˆå¤ªå¤§ï¼Œè«‹é¸æ“‡ 100MB ä»¥ä¸‹çš„æª”æ¡ˆ'
    event.target.value = ''
    return
  }
  
  selectedFile.value = file
  console.log(`æª”æ¡ˆé¸æ“‡: ${file.name}, é¡å‹: ${file.type}, å¤§å°: ${formatFileSize(file.size)}`)
  
  // è‡ªå‹•é–‹å§‹è½‰æ›
  await processUploadedFile(file)
  
  // æ¸…é™¤é¸æ“‡ä»¥å…è¨±é‡æ–°ä¸Šå‚³åŒä¸€æª”æ¡ˆ
  event.target.value = ''
}

// è™•ç†ä¸Šå‚³çš„æª”æ¡ˆ
const processUploadedFile = async (file) => {
  if (!selectedEngine.value) {
    errorMessage.value = 'è«‹å…ˆé¸æ“‡è½‰æ›å¼•æ“'
    return
  }
  
  isProcessing.value = true
  processingStatus.value = `æº–å‚™æª”æ¡ˆ... (${file.name})`
  processingProgress.value = 10
  
  try {
    const audioBlob = file
    const audioUrl = URL.createObjectURL(file)
    
    if (selectedEngine.value === 'webSpeech') {
      // Web Speech API ä¸æ”¯æ´æª”æ¡ˆä¸Šå‚³
      errorMessage.value = 'Web Speech API ä¸æ”¯æ´æª”æ¡ˆä¸Šå‚³ï¼Œè«‹ä½¿ç”¨ã€ŒğŸ¯ ç²¾æº–è½‰æ›ã€æˆ–ã€ŒğŸš€ è¶…å¼·è½‰æ›ã€å¼•æ“'
      isProcessing.value = false
      selectedFile.value = null
      return
    }

    let transcript
    
    if (selectedEngine.value === 'google') {
      if (!speechToTextService) {
        errorMessage.value = 'Google API æœå‹™æœªåˆå§‹åŒ–ï¼Œè«‹æª¢æŸ¥ API é‡‘é‘°è¨­å®š'
        isProcessing.value = false
        selectedFile.value = null
        return
      }
      
      console.log(`é–‹å§‹è½‰æ›: èªè¨€=${selectedLanguage.value}, å¼•æ“=Google`)
      
      // ä½¿ç”¨ Google Speech-to-Text API è½‰æ›
      processingStatus.value = `æ­£åœ¨è½‰æ› "${file.name}"...`
      processingProgress.value = 30
      
      transcript = await speechToTextService.transcribeAudio(
        audioBlob,
        selectedLanguage.value,
        (progress) => {
          processingStatus.value = `${progress.status} - ${file.name}`
          processingProgress.value = progress.progress
        }
      )
    } else if (selectedEngine.value === 'openai') {
      if (!openaiWhisperService) {
        errorMessage.value = 'OpenAI Whisper æœå‹™æœªåˆå§‹åŒ–ï¼Œè«‹æª¢æŸ¥ API é‡‘é‘°è¨­å®š'
        isProcessing.value = false
        selectedFile.value = null
        return
      }
      
      console.log(`é–‹å§‹è½‰æ›: èªè¨€=${selectedLanguage.value}, å¼•æ“=OpenAI Whisper`)
      
      // ä½¿ç”¨ OpenAI Whisper API è½‰æ› (æ”¯æ´å¤§æª”æ¡ˆåˆ†å‰²)
      processingStatus.value = `æ­£åœ¨è½‰æ› "${file.name}"...`
      processingProgress.value = 30
      
      // å°‡èªè¨€ä»£ç¢¼è½‰æ›ç‚º Whisper æ”¯æ´çš„æ ¼å¼ (ä¾‹å¦‚ zh-TW -> zh)
      const languageCode = selectedLanguage.value.split('-')[0]
      
      transcript = await openaiWhisperService.transcribeAudio(
        audioBlob,
        languageCode,
        (progress) => {
          processingStatus.value = `${progress.status} - ${file.name}`
          processingProgress.value = progress.progress
        }
      )
    }
    
    console.log(`è½‰æ›æˆåŠŸï¼Œçµæœé•·åº¦: ${transcript.length} å­—ç¬¦`)
    
    const recording = {
      audioUrl,
      audioBlob,
      transcript,
      summary: null,
      date: new Date().toLocaleString('zh-TW'),
      language: selectedLanguage.value,
      engine: selectedEngine.value,
      fileName: file.name
    }
    
    recordings.value.unshift(recording)
    selectedFile.value = null
    errorMessage.value = '' // æ¸…é™¤éŒ¯èª¤
    
  } catch (error) {
    console.error('æª”æ¡ˆè½‰æ›å¤±æ•—è©³ç´°ä¿¡æ¯:', {
      fileName: file.name,
      fileType: file.type,
      fileSize: file.size,
      error: error.message,
      stack: error.stack
    })
    
    errorMessage.value = `è½‰æ›å¤±æ•—: ${error.message}`
    selectedFile.value = null
    
    // å³ä½¿å¤±æ•—ä¹Ÿä¿å­˜éŒ„éŸ³ç´€éŒ„
    const recording = {
      audioUrl: URL.createObjectURL(file),
      audioBlob: file,
      transcript: `[è½‰æ›å¤±æ•—] ${error.message}\n\næª”æ¡ˆ: ${file.name}\né¡å‹: ${file.type}`,
      summary: null,
      date: new Date().toLocaleString('zh-TW'),
      language: selectedLanguage.value,
      engine: selectedEngine.value,
      fileName: file.name
    }
    recordings.value.unshift(recording)
    
  } finally {
    isProcessing.value = false
    processingStatus.value = ''
    processingProgress.value = 0
  }
}

// æ¸…ç†è³‡æº
onUnmounted(() => {
  if (recordingTimer) {
    clearInterval(recordingTimer)
  }
  recordings.value.forEach(recording => {
    if (recording.audioUrl) {
      URL.revokeObjectURL(recording.audioUrl)
    }
  })
})
</script>
